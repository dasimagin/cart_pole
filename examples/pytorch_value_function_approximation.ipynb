{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [Simulator directory](simulator_v2/cartpole_torch/) to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from cartpole.simulator.pytorch.config import SystemConfiguration\n",
    "from cartpole.simulator.pytorch.learning_context import MultiSystemLearningContext\n",
    "from cartpole.simulator.pytorch.system import CartPoleMultiSystem\n",
    "from cartpole.simulator.pytorch.state import State\n",
    "from cartpole.simulator.pytorch.discreditizer import Discreditizer\n",
    "import torch\n",
    "from torch import DoubleTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SystemConfiguration()\n",
    "system = CartPoleMultiSystem()\n",
    "discreditizer = Discreditizer(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define two cost functions: one for states, and one for inputs.\n",
    "Assume we are currently in state $S_1$ and are applying input $u$ to our model.\n",
    "After $\\delta t$ we will be in state $S_2$.\n",
    "\n",
    "The goal of the state cost function is to assign a penalty for the state $S_2$, and the goal of the input cost function is to assign a penalty for a given input.\n",
    "\n",
    "Notice, however, that our functions are slightly differrent from what we mentioned above.\n",
    "The first difference is that they are _vectorized_.\n",
    "It means that they work with multiple values at a time, which drastically improves the performance of Multisystem Simulation.\n",
    "Secondly, in `state_cost_fn` we have a huge punishment for system limits violation: if $x$ is greater than $x_{max}$, then we introduce a huge constant cost (50) for that state, which should teach the machine not to violate these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, theta, xdot, thetadot\n",
    "Q = 0.1 * torch.diag(DoubleTensor([2, 5, 1, 5]))  # type: ignore\n",
    "\n",
    "\n",
    "def state_cost_fn(states: DoubleTensor) -> DoubleTensor:\n",
    "    \"\"\"Returns an array with cost for each state\"\"\"\n",
    "    err = torch.clone(states)\n",
    "\n",
    "    # Change angle so that we are comparing against zero\n",
    "    err[1] %= 2 * torch.pi\n",
    "    err[1] -= torch.pi\n",
    "\n",
    "    # Punish for big position and speed\n",
    "    states = err[0, :]  # type: ignore\n",
    "    states[torch.abs(states) > config.limits.max_abs_position] = 50_000  # type: ignore\n",
    "    v = err[2, :]\n",
    "    v[torch.abs(v) > config.limits.max_abs_velocity] = 70_000\n",
    "\n",
    "    return (err * (Q @ err)).sum(axis=0)  # type: ignore\n",
    "\n",
    "\n",
    "def input_cost_fn(inputs: DoubleTensor) -> DoubleTensor:\n",
    "    return (inputs * inputs) / 5  # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFVI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement Continuous Neural Fitted Value Iteration algorithm in order to approximate the time-varying cost-to-go function $J$, which, given current state and time, returns us the minimal cost on an infinite horizon.\n",
    "We will use $J^*$ which is like $J$, but calculated on a finite horizon ($N$ steps).\n",
    "\n",
    "This function will be used for finding the optimal trajectory (the one with minimal cost)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to approximate it, we will take a sample from our continuous state space.\n",
    "Currently, it looks like a mesh with constant distance between the adjacent points, but this can be changed: for example, one might want a higher density in some region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = MultiSystemLearningContext(\n",
    "    states_cost_fn=state_cost_fn,\n",
    "    inputs_cost_fn=input_cost_fn,\n",
    "    config=config,\n",
    "    batch_state=None,\n",
    "    discreditizer=discreditizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested in [Chapter 7](http://underactuated.mit.edu/dp.html) of Underactuated Robotics by Russ Tedrake, we will solve our $J^*$ function backwards in time.\n",
    "\n",
    "We shall start with setting a horizon to $N$ and saying that $\\forall i\\ J^*(S_i, N) = \\min \\text{cost}(S_i, u)$, where $S_i$ is some state, $u$ is some input.\n",
    "In other words, we initialize $J^*$ simply with costs of the best inputs for each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_to_go = state_cost_fn(context.discreditizer.all_states)\n",
    "BATCH_SIZE = 5_000\n",
    "context.update_batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "index = cost_to_go.multinomial(num_samples=100_000, replacement=True)\n",
    "sns.histplot(cost_to_go[index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the `batch_size` to be 1000.\n",
    "This means that 1000 systems will be simulated at once.\n",
    "\n",
    "On every step, 1000 random states are taken from our sample, and best input is calculated for all of them.\n",
    "Then, keeping inputs constant, we perform multiple steps of dynamics calculation.\n",
    "This is needed to achieve greater precision of the simulation.\n",
    "\n",
    "All of the described actions are performed under the hood in `CartPoleMultiSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 5000\n",
    "\n",
    "for _ in tqdm(range(EPOCHS)):\n",
    "    inputs = system.get_best_accelerations(context)\n",
    "    new_states = system.eval_transitions(context, inputs)\n",
    "\n",
    "    costs = input_cost_fn(inputs)\n",
    "\n",
    "    ind = context.batch_indecies\n",
    "    cost_to_go[ind] = cost_to_go[ind] * GAMMA + costs\n",
    "\n",
    "    context.update_batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "torch.save(cost_to_go, \"cost_to_go.torchobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(cost_to_go[index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        # Here we have 5 input features, even though our state consists\n",
    "        # of 4 numbers. The explanation can be read in `forward` method.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(5, 32),  # input layer\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(32, 100),  # hidden layer\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(100, 1),  # output layer\n",
    "        )\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x: DoubleTensor) -> DoubleTensor:\n",
    "        # Here we transform our input data\n",
    "        # To be precise, instead of `pole_angle` (x[1]) we will use 2 features:\n",
    "        # `sin(pole_angle)` and `cos(pole_angle)` which allow us to fully determine\n",
    "        # the angle and also get rid of any issues connected with angle jumping from\n",
    "        # 2pi to 0 and back.\n",
    "        # So introducing sin and cos will make our function smoother.\n",
    "        x_transformed: DoubleTensor = torch.empty(\n",
    "            size=(x.shape[0], 5),\n",
    "            dtype=torch.double,\n",
    "        )\n",
    "        x_transformed[:, 0] = x[:, 0]\n",
    "        x_transformed[:, 1] = torch.sin(x[:, 1])\n",
    "        x_transformed[:, 2] = torch.cos(x[:, 1])\n",
    "        x_transformed[:, 3] = x[:, 2]\n",
    "        x_transformed[:, 4] = x[:, 3]\n",
    "\n",
    "        return self.layers(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.hstack((context.discreditizer.all_states.mT, cost_to_go.reshape(-1, 1)))\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=420,\n",
    "    shuffle=True,\n",
    ")\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "tol = 1\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(1, 19):\n",
    "    current_loss: float = 0\n",
    "\n",
    "    for i, data in enumerate(tqdm(trainloader)):\n",
    "        states: DoubleTensor = data[:, :4]\n",
    "        costs: DoubleTensor = data[:, 4]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicted_costs: DoubleTensor = model(states)\n",
    "        loss = loss_function(predicted_costs, costs.reshape(-1, 1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "\n",
    "    root_loss = sqrt(current_loss)\n",
    "\n",
    "    print(f\"Sqrt of average loss on epoch {epoch}: {float(root_loss)}\")\n",
    "    losses.append(float(root_loss))\n",
    "\n",
    "    if len(losses) >= 2 and abs(losses[-1] - losses[-2]) < tol:\n",
    "        print(\"Converged\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    y=losses,\n",
    "    x=np.arange(len(losses)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = State(\n",
    "    0,\n",
    "    3.05,\n",
    "    0,\n",
    "    0,\n",
    ")\n",
    "\n",
    "\n",
    "predicted_cost = float(model(example.as_tensor().reshape(1, -1)))\n",
    "print(\"Predicted cost for\", example, \"is\", predicted_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3beec89b79b7bce1dc5530492c5375aeea24672e6945dcc3acfe533c5e4de79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
